{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Misbehavior Detection in Vehicular Networks with Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-17T00:27:36.067645Z",
     "start_time": "2024-10-17T00:27:29.541158Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import locale\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC\n",
    "from metrics import *\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from itertools import cycle\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, auc\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.set_visible_devices(physical_devices[1:], 'GPU')\n",
    "  logical_devices = tf.config.list_logical_devices('GPU')\n",
    "  assert len(logical_devices) == len(physical_devices) - 1\n",
    "except:\n",
    "  pass\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')\n",
    "locale._override_localeconv = {'thousands_sep': '.'}\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:65% !important; }</style>\"))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 21:27:31.255889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 21:27:31.451272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 21:27:31.506211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:65% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:22:07.822046Z",
     "start_time": "2024-10-17T11:22:07.805804Z"
    }
   },
   "source": [
    "def load_veremi(csv_file: str, feature: str, label: str, delimiter=','):\n",
    "    # Import VeReMi Dataset\n",
    "    data = pd.read_csv(csv_file, delimiter=delimiter)\n",
    "\n",
    "    # select columns\n",
    "    columns = []\n",
    "    for column in data.columns.values:\n",
    "        if feature == 'feat1':\n",
    "            if 'RSSI' in column:\n",
    "                columns.append(column)\n",
    "            elif 'distance' in column:\n",
    "                columns.append(column)\n",
    "        elif feature == 'feat2':\n",
    "            if 'conformity' in column and '0' not in column:\n",
    "                columns.append(column)\n",
    "        elif feature == 'feat3':\n",
    "            if 'RSSI' in column and '0' not in column:\n",
    "                columns.append(column)\n",
    "            elif 'distance' in column and '0' not in column:\n",
    "                columns.append(column)\n",
    "            elif 'conformity' in column and '0' not in column:\n",
    "                columns.append(column)\n",
    "        elif feature == 'feat4':\n",
    "            if 'RSSI' in column:\n",
    "                columns.append(column)\n",
    "            elif 'aoa' in column:\n",
    "                columns.append(column)\n",
    "            elif 'distance' in column:\n",
    "                columns.append(column)\n",
    "            elif 'conformity' in column and '0' not in column:\n",
    "                columns.append(column)\n",
    "    columns.append('attack_type')\n",
    "\n",
    "    # process target values\n",
    "    if label == 'multiclass':\n",
    "        data = data[columns]\n",
    "    elif label == 'binary':\n",
    "        pos_label = 1\n",
    "        data = data[columns]\n",
    "        data['attack_type'].loc[data['attack_type'] != 0] = pos_label\n",
    "    else:\n",
    "        pos_label = int(label.split(\"_\")[1])\n",
    "        data = data[columns]\n",
    "        data = data.loc[(data['attack_type'] == 0) | (data['attack_type'] == pos_label)]\n",
    "\n",
    "    data_normal = data.loc[data['attack_type'] == 0]\n",
    "    data_atk = data.loc[data['attack_type'] != 0]\n",
    "    # atk_size = int(data_atk.shape[0] * 1.5)\n",
    "    atk_size = int(data_atk.shape[0])\n",
    "    data = pd.concat([data_normal.sample(atk_size), data_atk])\n",
    "    data = shuffle(data)\n",
    "\n",
    "    dataset = data\n",
    "    target = data[data.columns[-1:]]\n",
    "    data = data[data.columns[0:-1]]\n",
    "\n",
    "    # normalize data\n",
    "    data = (data - data.mean()) / data.std()\n",
    "\n",
    "    # label binarize one-hot style\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(target)\n",
    "    if label == 'multiclass':\n",
    "        target = lb.transform(target)\n",
    "    else:\n",
    "        target = lb.transform(target)\n",
    "        target = MultiLabelBinarizer().fit_transform(target)\n",
    "\n",
    "    # Create training and test data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        data,\n",
    "        target,\n",
    "        train_size=Config.data_train_size,\n",
    "        test_size=Config.data_test_size,\n",
    "        # random_state=42\n",
    "    )\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels, lb, dataset"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T00:27:48.478221Z",
     "start_time": "2024-10-17T00:27:48.473765Z"
    }
   },
   "source": [
    "class Config:\n",
    "    csv = \"./VeReMi.csv\"\n",
    "    model_type = \"mlp\"\n",
    "    label = \"multiclass\"\n",
    "    feature = \"feat4\"\n",
    "    batch_size = 128\n",
    "    epochs = 50\n",
    "    rounds = 80\n",
    "    learning_rate = 3e-4 #1e-3\n",
    "    min_available_clients = 2\n",
    "    fraction_fit = 0.1\n",
    "    early_stop_patience = 3\n",
    "    early_stop_monitor = \"loss\"\n",
    "    early_stop_min_delta = 1e-4\n",
    "    early_stop_restore_best_weights = True\n",
    "    data_train_size = 0.8\n",
    "    data_test_size = 0.2\n",
    "    output_path = f\"results/{feature}/{label}/\"\n",
    "    performance_file = \"performance.csv\"\n",
    "    weights_file = \"model_weights.npz\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T00:27:50.588746Z",
     "start_time": "2024-10-17T00:27:50.582399Z"
    }
   },
   "source": [
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VeReMi Base Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T00:27:52.528607Z",
     "start_time": "2024-10-17T00:27:52.519097Z"
    }
   },
   "source": [
    "class VeremiBase(ABC):\n",
    "    def __init__(self, data_file: str, model_type: str, label: str, feature: str, activation: str = \"softmax\"):\n",
    "        \"\"\" The Veremi Client Constructor\n",
    "            :param model_type: Keras Model Type ('mlp' or 'lstm'\n",
    "            :param label: Model label type ('binary', 'multiclass', 'atk_1', 'atk_2', 'atk_4', 'atk_8', 'atk_16')\n",
    "            :param feature: Feature to evaluate ('feat1', 'feat2', 'feat3')\n",
    "        \"\"\"\n",
    "        self.lb = None\n",
    "        self.dataset = None\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.train_labels = None\n",
    "        self.test_labels = None\n",
    "        self.model = None\n",
    "        self.data_file = data_file\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.model_type = model_type\n",
    "        self.activation = activation\n",
    "\n",
    "        self.load_veremi()\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        layer1, layer2, layer3, layer4, layer5, layer6, layer7, layer8, layer9, output = \\\n",
    "            None, None, None, None, None, None, None, None, None, None\n",
    "        if self.model_type == 'mlp':\n",
    "            layer1 = keras.layers.Input(shape=(self.train_data.shape[1],))\n",
    "            layer2 = keras.layers.Dense(256, activation=\"relu\")(layer1)\n",
    "            layer3 = keras.layers.Dense(256, activation=\"relu\")(layer2)\n",
    "            layer4 = keras.layers.Dropout(0.5)(layer3)\n",
    "            output = keras.layers.Dense(self.train_labels.shape[1], activation=self.activation)(layer4)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # ML Model\n",
    "        name = self.label + \"-\" + self.model_type + \"-\" + self.feature\n",
    "        self.model = keras.Model(inputs=layer1, outputs=output, name=name)\n",
    "        self.model.compile(\n",
    "            loss=keras.losses.CategoricalCrossentropy(),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n",
    "            metrics=[f1]\n",
    "        )\n",
    "        self.model.summary()\n",
    "\n",
    "    def load_veremi(self):\n",
    "        print(\"Loading dataset in \" + self.__class__.__name__ + \"...\")\n",
    "        self.train_data, self.test_data, self.train_labels, self.test_labels, self.lb, self.dataset = load_veremi(\n",
    "            self.data_file,\n",
    "            feature=self.feature,\n",
    "            label=self.label\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VeReMi Client Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T00:27:54.924539Z",
     "start_time": "2024-10-17T00:27:54.914821Z"
    }
   },
   "source": [
    "class VeremiClient(VeremiBase, fl.client.NumPyClient):\n",
    "    def __init__(self, data_file: str, model_type: str, label: str, feature: str):\n",
    "        VeremiBase.__init__(self, data_file, model_type, label, feature)\n",
    "        self.history = None\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def fit(self, parameters, config, verbose):\n",
    "        print(\"Training...\")\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=Config.early_stop_monitor,\n",
    "            patience=Config.early_stop_patience,\n",
    "            min_delta=Config.early_stop_min_delta,\n",
    "            restore_best_weights=Config.early_stop_restore_best_weights\n",
    "        )\n",
    "        if parameters is not None:\n",
    "            self.model.set_weights(parameters)\n",
    "        self.history = self.model.fit(\n",
    "            self.train_data,\n",
    "            self.train_labels,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            epochs=config[\"epochs\"],\n",
    "            # callbacks=[early_stopping],\n",
    "            validation_data=(self.test_data, self.test_labels),\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        result = {\n",
    "            \"f1_score:\": float(self.history.history['f1'][-1]),\n",
    "            \"f1_val\": float(self.history.history['val_f1'][-1]),\n",
    "        }\n",
    "        print(\"Finished!\")\n",
    "        return self.model.get_weights(), len(self.train_data), result"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Client"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-10-17T11:22:16.655318Z",
     "start_time": "2024-10-17T11:22:15.734603Z"
    }
   },
   "source": [
    "client = VeremiClient(Config.csv, Config.model_type, Config.label, Config.feature)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset in VeremiClient...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m client \u001B[38;5;241m=\u001B[39m \u001B[43mVeremiClient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m, in \u001B[0;36mVeremiClient.__init__\u001B[0;34m(self, data_file, model_type, label, feature)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_file: \u001B[38;5;28mstr\u001B[39m, model_type: \u001B[38;5;28mstr\u001B[39m, label: \u001B[38;5;28mstr\u001B[39m, feature: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mVeremiBase\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhistory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 21\u001B[0m, in \u001B[0;36mVeremiBase.__init__\u001B[0;34m(self, data_file, model_type, label, feature, activation)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_type \u001B[38;5;241m=\u001B[39m model_type\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation \u001B[38;5;241m=\u001B[39m activation\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_veremi\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_model()\n",
      "Cell \u001B[0;32mIn[6], line 48\u001B[0m, in \u001B[0;36mVeremiBase.load_veremi\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_veremi\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading dataset in \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlb, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_veremi\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 50\u001B[0m, in \u001B[0;36mload_veremi\u001B[0;34m(csv_file, feature, label, delimiter)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# atk_size = int(data_atk.shape[0] * 1.5)\u001B[39;00m\n\u001B[1;32m     49\u001B[0m atk_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(data_atk\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 50\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[43mdata_normal\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43matk_size\u001B[49m\u001B[43m)\u001B[49m, data_atk])\n\u001B[1;32m     51\u001B[0m data \u001B[38;5;241m=\u001B[39m shuffle(data)\n\u001B[1;32m     53\u001B[0m dataset \u001B[38;5;241m=\u001B[39m data\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/ic-veremi-fed-learning-Xdbt6niO/lib/python3.10/site-packages/pandas/core/generic.py:6118\u001B[0m, in \u001B[0;36mNDFrame.sample\u001B[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001B[0m\n\u001B[1;32m   6115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   6116\u001B[0m     weights \u001B[38;5;241m=\u001B[39m sample\u001B[38;5;241m.\u001B[39mpreprocess_weights(\u001B[38;5;28mself\u001B[39m, weights, axis)\n\u001B[0;32m-> 6118\u001B[0m sampled_indices \u001B[38;5;241m=\u001B[39m \u001B[43msample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6119\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(sampled_indices, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[1;32m   6121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ignore_index:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/ic-veremi-fed-learning-Xdbt6niO/lib/python3.10/site-packages/pandas/core/sample.py:152\u001B[0m, in \u001B[0;36msample\u001B[0;34m(obj_len, size, replace, weights, random_state)\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    150\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid weights: weights sum to zero\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mastype(\n\u001B[1;32m    153\u001B[0m     np\u001B[38;5;241m.\u001B[39mintp, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    154\u001B[0m )\n",
      "File \u001B[0;32mnumpy/random/mtrand.pyx:945\u001B[0m, in \u001B[0;36mnumpy.random.mtrand.RandomState.choice\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VeReMi DataSet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "client.dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "client.train_data.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "client.dataset.plot.scatter(x='aoa0', y='aoa1', c='attack_type', colormap='viridis')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "attack = len(client.dataset[client.dataset.attack_type != 0])\n",
    "normal = len(client.dataset[client.dataset.attack_type == 0])\n",
    "total = attack + normal\n",
    "print('Attackers:\\n    Total: {:,d}\\n    Attack: {:,d} ({:.2f}% of total)\\n'.format(\n",
    "    total, attack, 100 * attack / total))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "atk_df = client.dataset.loc[client.dataset['attack_type'] != 0].sample(128)\n",
    "normal_df = client.dataset.loc[client.dataset['attack_type'] == 0].sample(128)\n",
    "\n",
    "# normalize data\n",
    "columns = atk_df.columns[0:-1]\n",
    "atk_df[columns] = (atk_df[columns] - atk_df[columns].mean()) / atk_df[columns].std()\n",
    "\n",
    "columns = normal_df.columns[0:-1]\n",
    "normal_df[columns] = (normal_df[columns] - normal_df[columns].mean()) / normal_df[columns].std()\n",
    "\n",
    "atk_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "atk_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "normal_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Plot\n",
    "sns.jointplot(x=atk_df['RSSI0'], y=atk_df['distance0'], kind='hex')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Plot\n",
    "sns.jointplot(x=atk_df['RSSI1'], y=atk_df['distance1'], kind='hex')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Params"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "initial_parameters = None\n",
    "file = Config.output_path + Config.weights_file\n",
    "if os.path.exists(file):\n",
    "    npzfile = np.load(file)\n",
    "    params = [npzfile[x] for x in npzfile]\n",
    "    params = fl.common.ndarrays_to_parameters(params)\n",
    "    initial_parameters = fl.common.parameters_to_ndarrays(params)\n",
    "    print(\"Setting model params...\")\n",
    "    client.model.set_weights(initial_parameters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "results = client.fit(\n",
    "    parameters=None, # initial_parameters,\n",
    "    config={\n",
    "        \"batch_size\": Config.batch_size,\n",
    "        \"epochs\": Config.epochs\n",
    "    },\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train_f1_score = client.history.history['f1']\n",
    "test_f1_score = client.history.history['val_f1']\n",
    "train_loss = client.history.history['loss']\n",
    "test_loss = client.history.history['val_loss']\n",
    "epochs = range(1, Config.epochs + 1)\n",
    "loss, num_examples, metrics = results\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.suptitle('Model Performance')\n",
    "fig.set_figwidth(15)\n",
    "fig.set_figheight(5)\n",
    "\n",
    "ax1.plot(epochs, train_loss, '-g', label=\"Train Loss\")\n",
    "ax1.plot(epochs, test_loss, '-b', label=\"Test Loss\")\n",
    "ax1.legend()\n",
    "ax1.set(xlabel='Epochs', ylabel='Loss')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(epochs, train_f1_score, '-g', label=\"Train F1 Score\")\n",
    "ax2.plot(epochs, test_f1_score, '-b', label=\"Test F1 Score\")\n",
    "ax2.legend()\n",
    "ax2.set(xlabel='Epochs', ylabel='F1 Score')\n",
    "ax2.set_title('F1-Score')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_pr_roc_curves(probabilities: Any):\n",
    "    n_classes = client.test_labels.shape[1]\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    pr_auc = dict()\n",
    "        \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(client.test_labels[:, i], probabilities[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        precision[i], recall[i], _ = precision_recall_curve(client.test_labels[:, i], probabilities[:, i])\n",
    "        pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_precision = np.unique(np.concatenate([precision[i] for i in range(n_classes)]))\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    mean_recall = np.zeros_like(all_precision)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_recall += np.interp(all_precision, precision[i], recall[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    mean_recall /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    precision[\"macro\"] = all_precision\n",
    "    recall[\"macro\"] = mean_recall\n",
    "    pr_auc[\"macro\"] = auc(recall[\"macro\"], precision[\"macro\"])\n",
    "\n",
    "    lw = 2\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    fig.suptitle('PR and ROC Curves - Multiclass')\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    colors = cycle([\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"])\n",
    "    \n",
    "    # ROC Curve\n",
    "    ax1.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=\"Macro Avg (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "        alpha=0.5,        \n",
    "    )\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        label_classes = int(client.lb.classes_[i])\n",
    "        ax1.plot(\n",
    "            fpr[i],\n",
    "            tpr[i],\n",
    "            color=color,\n",
    "            lw=lw,\n",
    "            label=\"Class {0} (area = {1:0.2f})\".format(label_classes, roc_auc[i]),\n",
    "            alpha=0.5\n",
    "        )\n",
    "    ax1.set_title(f\"ROC Curve - {Config.feature} - {Config.label}\")\n",
    "    ax1.set_xlabel(\"False Positive Rate\")\n",
    "    ax1.set_ylabel(\"True Positive Rate\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # PR curve\n",
    "    ax2.plot(\n",
    "        precision[\"macro\"],\n",
    "        recall[\"macro\"],\n",
    "        label=\"Macro Avg (area = {0:0.2f})\".format(pr_auc[\"macro\"]),\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        label_classes = int(client.lb.classes_[i])\n",
    "        ax2.plot(\n",
    "            precision[i],\n",
    "            recall[i],\n",
    "            color=color,\n",
    "            lw=lw,\n",
    "            label=\"Class {0} (area = {1:0.2f})\".format(label_classes, pr_auc[i]),\n",
    "            alpha=0.5\n",
    "        )\n",
    "    ax2.set_title(f\"PR Curve - {Config.feature} - {Config.label}\")    \n",
    "    ax2.set_xlabel(\"False Positive Rate\")\n",
    "    ax2.set_ylabel(\"True Positive Rate\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "probabilities = client.model.predict(client.test_data)\n",
    "inverse_target = client.lb.inverse_transform(client.test_labels)\n",
    "prediction = None\n",
    "\n",
    "if Config.label == 'multiclass':\n",
    "    prediction = client.lb.inverse_transform(probabilities)\n",
    "    # TODO: PLOT MULTICLASS \n",
    "    plot_pr_roc_curves(probabilities)\n",
    "else:\n",
    "    pos_label = 1 if Config.label == 'binary' else int(Config.label.split(\"_\")[1])\n",
    "    # Best threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        inverse_target,\n",
    "        probabilities[:, 1],\n",
    "        pos_label=pos_label\n",
    "    )\n",
    "    # convert to f score\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    np.nan_to_num(fscore, copy=False)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    prediction = np.where(np.array(probabilities[:, 1]) >= thresholds[ix], pos_label, 0)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "    fig.suptitle('PR and ROC Curves')\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    # PR Curve\n",
    "    PrecisionRecallDisplay.from_predictions(inverse_target, probabilities[:, 1], pos_label=pos_label, ax=ax1)\n",
    "    no_skill = len(inverse_target[inverse_target == 1]) / len(inverse_target)\n",
    "    ax1.plot([0, 1], [no_skill, no_skill], linestyle='--', color=\"grey\", label='No Skill')\n",
    "    ax1.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best threshold')\n",
    "    ax1.set_title(f\"PR Curve - {Config.feature} - {Config.label}\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # ROC curve\n",
    "    RocCurveDisplay.from_predictions(inverse_target, probabilities[:, 1], pos_label=pos_label, ax=ax2)\n",
    "    ax2.plot([0, 1], [0, 1], color=\"grey\", lw=1, linestyle=\"--\")\n",
    "    ax2.set_title(f\"ROC Curve - {Config.feature} - {Config.label}\")\n",
    "    \n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "classlist = []\n",
    "for cl in client.lb.classes_:\n",
    "    classlist.append('class ' + str(int(cl)))\n",
    "\n",
    "print(classification_report(inverse_target, prediction, target_names=classlist, digits=3, zero_division=0))\n",
    "print(\"-\" * 70)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cm = confusion_matrix(inverse_target, prediction, labels=client.lb.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=client.lb.classes_)\n",
    "disp.plot()\n",
    "plt.title(f\"Confusion Matrix - {Config.feature} - {Config.label}\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "name = Config.label + \"-\" + Config.model_type + \"-\" + Config.feature\n",
    "if Config.label == 'multiclass':\n",
    "    prscore = precision_score(inverse_target, prediction, average='macro', zero_division=0)\n",
    "    rcscore = recall_score(inverse_target, prediction, average='macro', zero_division=0)\n",
    "    f1score = f1_score(inverse_target, prediction, average='macro', zero_division=0)\n",
    "    accscore = accuracy_score(inverse_target, prediction)\n",
    "else:\n",
    "    prscore = precision_score(inverse_target, prediction, pos_label=pos_label, zero_division=0)\n",
    "    rcscore = recall_score(inverse_target, prediction, pos_label=pos_label, zero_division=0)\n",
    "    f1score = f1_score(inverse_target, prediction, pos_label=pos_label, zero_division=0)\n",
    "    accscore = accuracy_score(inverse_target, prediction)\n",
    "data_performance = {name: [prscore, rcscore, f1score, accscore]}\n",
    "df_performance = pd.DataFrame.from_dict(data_performance, orient='index', columns=[\"precision\", \"recall\", \"f1score\", \"accuracy\"])\n",
    "df_performance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
